"""
AIå®¢æˆ·ç«¯æ¨¡å— - å¤„ç†ä¸AI APIçš„äº¤äº’
"""

import os
import json
import requests
import threading
import time
import queue
import sys
from concurrent.futures import ThreadPoolExecutor, Future
from .thinking_animation import start_thinking, stop_thinking
from .keyboard_handler import (
    start_task_monitoring, stop_task_monitoring,
    is_task_interrupted, reset_interrupt_flag,
    interrupt_current_task
)
from .output_monitor import start_output_monitoring, stop_output_monitoring, enable_print_monitoring
from .config import load_config, DEFAULT_API_URL
from .debug_config import is_raw_output_enabled
from colorama import Fore, Style

def format_ai_response(raw_response, api_result=None):
    """
    æ ¹æ®è°ƒè¯•é…ç½®æ ¼å¼åŒ–AIå“åº”

    Args:
        raw_response (str): AIçš„åŸå§‹å“åº”å†…å®¹
        api_result (dict, optional): å®Œæ•´çš„APIå“åº”ç»“æœ

    Returns:
        str: æ ¼å¼åŒ–åçš„å“åº”å†…å®¹
    """
    if is_raw_output_enabled():
        # åŸå§‹è¾“å‡ºæ¨¡å¼ï¼šæ˜¾ç¤ºå®Œæ•´çš„æœªç»å¤„ç†çš„æ•°æ®
        output_lines = []
        output_lines.append("=" * 80)
        output_lines.append("ğŸ”§ åŸå§‹è¾“å‡ºæ¨¡å¼ - è°ƒè¯•ä¿¡æ¯")
        output_lines.append("=" * 80)

        if api_result:
            output_lines.append("\nğŸ“¡ å®Œæ•´APIå“åº”:")
            output_lines.append("-" * 40)
            import json
            try:
                formatted_json = json.dumps(api_result, indent=2, ensure_ascii=False)
                output_lines.append(formatted_json)
            except:
                output_lines.append(str(api_result))

        output_lines.append("\nğŸ’¬ AIåŸå§‹å“åº”å†…å®¹:")
        output_lines.append("-" * 40)
        output_lines.append(raw_response)

        output_lines.append("\n" + "=" * 80)
        output_lines.append("ğŸ”§ åŸå§‹è¾“å‡ºæ¨¡å¼ç»“æŸ")
        output_lines.append("=" * 80)

        return "\n".join(output_lines)
    else:
        # æ­£å¸¸æ¨¡å¼ï¼šè¿”å›æ¸²æŸ“åçš„ç”¨æˆ·å‹å¥½å†…å®¹
        return raw_response

def timeout_protection(timeout_seconds=200):
    """è¶…æ—¶ä¿æŠ¤è£…é¥°å™¨"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            result = [None]
            exception = [None]

            def target():
                try:
                    result[0] = func(*args, **kwargs)
                except Exception as e:
                    exception[0] = e

            thread = threading.Thread(target=target, daemon=True)
            thread.start()
            thread.join(timeout=timeout_seconds)

            if thread.is_alive():
                # è¶…æ—¶äº†ï¼Œå¼ºåˆ¶æ¸…ç†
                try:
                    stop_thinking()
                    stop_task_monitoring()
                except:
                    pass
                return "è¯·æ±‚è¶…æ—¶ï¼Œå·²å¼ºåˆ¶åœæ­¢ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•ã€‚"

            if exception[0]:
                raise exception[0]

            return result[0]
        return wrapper
    return decorator

class AsyncNetworkManager:
    """å¼‚æ­¥ç½‘ç»œè¯·æ±‚ç®¡ç†å™¨"""

    def __init__(self):
        self.executor = ThreadPoolExecutor(max_workers=2)
        self.current_request = None

    def submit_request(self, func, *args, **kwargs):
        """æäº¤å¼‚æ­¥è¯·æ±‚"""
        if self.current_request and not self.current_request.done():
            # å–æ¶ˆä¹‹å‰çš„è¯·æ±‚
            self.current_request.cancel()

        self.current_request = self.executor.submit(func, *args, **kwargs)
        return self.current_request

    def check_result(self, timeout=0.1):
        """éé˜»å¡æ£€æŸ¥ç»“æœ"""
        if self.current_request is None:
            return None, False

        try:
            result = self.current_request.result(timeout=timeout)
            return result, True
        except Exception as e:
            if self.current_request.done():
                return str(e), True
            return None, False

    def cancel_current_request(self):
        """å–æ¶ˆå½“å‰è¯·æ±‚"""
        if self.current_request and not self.current_request.done():
            self.current_request.cancel()
            return True
        return False

    def shutdown(self):
        """å…³é—­çº¿ç¨‹æ± """
        self.executor.shutdown(wait=False)

class AIClient:
    """AIå®¢æˆ·ç«¯ç±»"""

    def __init__(self):
        self.config = load_config()
        self.api_url = DEFAULT_API_URL
        self.conversation_history = []
        self.is_loading = False
        self.loading_thread = None
        self.network_manager = AsyncNetworkManager()
        
        # é›†æˆæ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        from .context_manager import context_manager
        self.context_manager = context_manager
        
        # é›†æˆä»£ç†å¼ç¼–ç¨‹å¢å¼ºå™¨
        from .agent_enhancer import agent_enhancer
        self.agent_enhancer = agent_enhancer

    def get_system_prompt(self):
        """è·å–ç³»ç»Ÿæç¤ºè¯"""
        # æ£€æŸ¥å½“å‰æ¨¡å¼å’Œæç¤ºè¯å¼ºåº¦
        from .modes import mode_manager
        from .config import load_config
        from .byteiq_config import byteiq_config_manager

        current_mode = mode_manager.get_current_mode()
        config = load_config()
        prompt_strength = config.get('prompt_strength', 'claude')  # é»˜è®¤ä½¿ç”¨claudeå¼ºåº¦

        # å¯¼å…¥æç¤ºè¯æ¨¡æ¿ç³»ç»Ÿ
        from .prompt_templates import get_prompt_template

        base_prompt = get_prompt_template(current_mode, prompt_strength)
        
        # ä½¿ç”¨BYTEIQ.mdé…ç½®å¢å¼ºæç¤ºè¯
        enhanced_prompt = byteiq_config_manager.get_enhanced_system_prompt(base_prompt)
        
        return enhanced_prompt





    def get_project_structure(self, path=".", max_depth=3, current_depth=0):
        """è·å–é¡¹ç›®ç»“æ„"""
        if current_depth >= max_depth:
            return ""

        structure = ""
        try:
            items = sorted(os.listdir(path))
            for item in items:
                if item.startswith('.'):
                    continue

                item_path = os.path.join(path, item)
                indent = "  " * current_depth

                if os.path.isdir(item_path):
                    structure += f"{indent}{item}/\n"
                    structure += self.get_project_structure(item_path, max_depth, current_depth + 1)
                else:
                    structure += f"{indent}{item}\n"
        except PermissionError:
            pass

        return structure

    # æ—§çš„åŠ è½½åŠ¨ç”»å·²ç§»é™¤ï¼Œä½¿ç”¨æ–°çš„æ€è€ƒåŠ¨ç”»ç³»ç»Ÿ

    def _make_network_request(self, data, headers):
        """æ‰§è¡Œç½‘ç»œè¯·æ±‚ï¼ˆåœ¨å­çº¿ç¨‹ä¸­è¿è¡Œï¼‰"""
        try:
            response = requests.post(self.api_url, json=data, headers=headers, timeout=180)
            if response.status_code == 401:
                return {"error": "APIå¯†é’¥æ— æ•ˆæˆ–æœªæˆæƒã€‚è¯·æ£€æŸ¥æ‚¨çš„å¯†é’¥ã€‚", "status_code": 401}
            response.raise_for_status()
            return response.json()
        except requests.exceptions.HTTPError as e:
            return {"error": f"HTTP é”™è¯¯: {e.response.status_code} - {e.response.text}"}
        except Exception as e:
            return {"error": str(e)}

    def send_message_async(self, user_input, include_structure=True, model_override=None):
        """å¼‚æ­¥å‘é€æ¶ˆæ¯ï¼Œè¿”å›Futureå¯¹è±¡"""
        config = load_config()

        if not config.get('api_key'):
            return None

        # å†³å®šä½¿ç”¨å“ªä¸ªæ¨¡å‹
        model_to_use = model_override if model_override else config.get('model', 'gpt-3.5-turbo')

        # æ„å»ºè¯·æ±‚æ•°æ®
        data = {
            "model": model_to_use,
            "messages": [
                {"role": "system", "content": self.get_system_prompt()},
                *self.conversation_history,
                {"role": "user", "content": user_input}
            ],
            "temperature": 0.7,
            "max_tokens": 6000
        }

        headers = {
            "Authorization": f"Bearer {config['api_key']}",
            "Content-Type": "application/json"
        }

        # æäº¤å¼‚æ­¥è¯·æ±‚
        return self.network_manager.submit_request(self._make_network_request, data, headers)

    def send_message_streaming(self, user_input, include_structure=True, model_override=None, is_continuation=False):
        """æµå¼å‘é€æ¶ˆæ¯ç»™AIï¼Œå®æ—¶æ˜¾ç¤ºå“åº”"""
        config = load_config()

        if not config.get('api_key'):
            return "é”™è¯¯ï¼šè¯·å…ˆè®¾ç½®APIå¯†é’¥"

        # å†³å®šä½¿ç”¨å“ªä¸ªæ¨¡å‹
        model_to_use = model_override if model_override else config.get('model', 'gpt-3.5-turbo')

        # æ„å»ºè¯·æ±‚æ•°æ®
        data = {
            "model": model_to_use,
            "messages": [
                {"role": "system", "content": self.get_system_prompt()},
                *self.conversation_history,
                {"role": "user", "content": user_input}
            ],
            "temperature": 0.7,
            "max_tokens": 6000,
            "stream": True  # å¯ç”¨æµå¼è¾“å‡º
        }

        headers = {
            "Authorization": f"Bearer {config['api_key']}",
            "Content-Type": "application/json"
        }

        # å¯åŠ¨ä»»åŠ¡ç›‘æ§
        start_task_monitoring(interrupt_current_task)

        try:
            print(f"{Fore.GREEN}AI: {Style.RESET_ALL}", end="", flush=True)
            
            # å‘é€æµå¼è¯·æ±‚
            response = requests.post(self.api_url, json=data, headers=headers, stream=True, timeout=180)
            
            if response.status_code == 401:
                return f"è®¤è¯å¤±è´¥: APIå¯†é’¥æ— æ•ˆæˆ–æœªæˆæƒã€‚è¯·æ£€æŸ¥æ‚¨çš„å¯†é’¥ã€‚"
            
            if response.status_code != 200:
                return f"APIè¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}"

            full_response = ""
            
            # é€è¡Œå¤„ç†æµå¼å“åº”
            for line in response.iter_lines():
                # æ£€æŸ¥ç”¨æˆ·ä¸­æ–­
                if is_task_interrupted():
                    reset_interrupt_flag()
                    print(f"\n{Fore.YELLOW}[ä»»åŠ¡å·²è¢«ç”¨æˆ·ä¸­æ–­]{Style.RESET_ALL}")
                    return "ä»»åŠ¡å·²è¢«ç”¨æˆ·ä¸­æ–­"
                
                if line:
                    line_str = line.decode('utf-8')
                    if line_str.startswith('data: '):
                        data_str = line_str[6:]  # ç§»é™¤ 'data: ' å‰ç¼€
                        
                        if data_str.strip() == '[DONE]':
                            break
                        
                        try:
                            chunk_data = json.loads(data_str)
                            if 'choices' in chunk_data and len(chunk_data['choices']) > 0:
                                delta = chunk_data['choices'][0].get('delta', {})
                                if 'content' in delta:
                                    content = delta['content']
                                    print(content, end="", flush=True)
                                    full_response += content
                        except json.JSONDecodeError:
                            continue
            
            print()  # æ¢è¡Œ
            
            # æ·»åŠ åˆ°å¯¹è¯å†å²
            self.conversation_history.append({"role": "user", "content": user_input})
            self.conversation_history.append({"role": "assistant", "content": full_response})

            # é™åˆ¶å†å²é•¿åº¦ï¼Œä½†ä¿ç•™æ›´å¤šä¸Šä¸‹æ–‡ï¼ˆä»10å¢åŠ åˆ°20ï¼‰
            if len(self.conversation_history) > 20:
                self.conversation_history = self.conversation_history[-20:]

            return full_response

        except requests.exceptions.Timeout:
            return "è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•"
        except requests.exceptions.RequestException as e:
            return f"ç½‘ç»œé”™è¯¯: {str(e)}"
        except Exception as e:
            return f"å‘ç”Ÿé”™è¯¯: {str(e)}"
        finally:
            # ç¡®ä¿åœæ­¢ç›‘æ§
            try:
                stop_task_monitoring()
            except:
                pass

    def send_message_non_blocking(self, user_input, include_structure=True, model_override=None):
        """éé˜»å¡å‘é€æ¶ˆæ¯ç»™AI"""
        # å¯åŠ¨æ€è€ƒåŠ¨ç”»
        start_thinking()

        # å¯åŠ¨ä»»åŠ¡ç›‘æ§
        start_task_monitoring(interrupt_current_task)

        try:
            # æäº¤å¼‚æ­¥è¯·æ±‚
            future = self.send_message_async(user_input, include_structure, model_override)
            if not future:
                return "é”™è¯¯ï¼šè¯·å…ˆè®¾ç½®APIå¯†é’¥"

            # éé˜»å¡ç­‰å¾…ç»“æœ
            start_time = time.time()
            max_wait_time = 180  # æœ€å¤§ç­‰å¾…180ç§’

            while True:
                # æ£€æŸ¥ç”¨æˆ·ä¸­æ–­
                if is_task_interrupted():
                    self.network_manager.cancel_current_request()
                    reset_interrupt_flag()
                    return "ä»»åŠ¡å·²è¢«ç”¨æˆ·ä¸­æ–­"

                # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
                if time.time() - start_time > max_wait_time:
                    self.network_manager.cancel_current_request()
                    return "è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"

                # éé˜»å¡æ£€æŸ¥ç»“æœ
                result, is_done = self.network_manager.check_result(timeout=0.1)

                if is_done:
                    if isinstance(result, dict) and "error" in result:
                        if result.get("status_code") == 401:
                            return f"è®¤è¯å¤±è´¥: {result['error']}"
                        return f"ç½‘ç»œé”™è¯¯: {result['error']}"

                    # å¤„ç†æˆåŠŸå“åº”
                    if isinstance(result, dict) and "choices" in result:
                        ai_response = result["choices"][0]["message"]["content"]

                        # æ·»åŠ åˆ°å¯¹è¯å†å²
                        self.conversation_history.append({"role": "user", "content": user_input})
                        self.conversation_history.append({"role": "assistant", "content": ai_response})

                        # é™åˆ¶å†å²é•¿åº¦ï¼Œä½†ä¿ç•™æ›´å¤šä¸Šä¸‹æ–‡ï¼ˆä»10å¢åŠ åˆ°20ï¼‰
                        if len(self.conversation_history) > 20:
                            self.conversation_history = self.conversation_history[-20:]

                        # æ ¹æ®è°ƒè¯•é…ç½®æ ¼å¼åŒ–å“åº”
                        return format_ai_response(ai_response, result)
                    else:
                        return f"APIå“åº”æ ¼å¼é”™è¯¯: {result}"

                # çŸ­æš‚ä¼‘çœ ï¼Œé¿å…CPUå ç”¨è¿‡é«˜
                time.sleep(0.1)

        except Exception as e:
            return f"å‘ç”Ÿé”™è¯¯: {str(e)}"
        finally:
            # ç¡®ä¿åœæ­¢åŠ¨ç”»å’Œç›‘æ§
            try:
                stop_thinking()
                stop_task_monitoring()
            except:
                pass

    @timeout_protection(timeout_seconds=200)
    def send_message(self, user_input, include_structure=True):
        """å‘é€æ¶ˆæ¯ç»™AIï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
        try:
            # åˆ†æç”¨æˆ·è¯·æ±‚å¹¶åˆ›å»ºæ‰§è¡Œè®¡åˆ’
            analysis = self.agent_enhancer.analyze_user_request(user_input)
            
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°ä¸Šä¸‹æ–‡ç®¡ç†å™¨
            self.context_manager.add_message("user", user_input, {"analysis": analysis})
            
            # å¦‚æœéœ€è¦è§„åˆ’ï¼Œåˆ›å»ºæ‰§è¡Œè®¡åˆ’
            if analysis["requires_planning"]:
                plan_result = self.agent_enhancer.create_execution_plan(user_input, analysis)
                self.context_manager.add_project_context("current_plan", plan_result, "high")
            
            # æ„å»ºç”¨æˆ·æ¶ˆæ¯
            user_message = user_input
            if include_structure:
                structure = self.get_project_structure()
                if structure.strip():
                    user_message += f"\n\nå½“å‰é¡¹ç›®ç»“æ„ï¼š\n```\n{structure}```"
                    # æ·»åŠ é¡¹ç›®ç»“æ„åˆ°ä¸Šä¸‹æ–‡
                    self.context_manager.add_project_context("project_structure", structure, "high")
                else:
                    user_message += "\n\nå½“å‰é¡¹ç›®ç»“æ„ï¼šç©º"

            # è·å–ç³»ç»Ÿæç¤ºè¯å¹¶æ ¹æ®æ€è€ƒæ¨¡å¼å¢å¼º
            base_prompt = self.get_system_prompt()
            thinking_mode = analysis["thinking_mode"]
            enhanced_prompt = self.agent_enhancer.enhance_prompt_with_thinking(base_prompt, thinking_mode)

            # ä½¿ç”¨æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†å™¨è·å–æ¶ˆæ¯
            messages = [{"role": "system", "content": enhanced_prompt}]
            
            # è·å–å¢å¼ºçš„æ¶ˆæ¯å†å²
            enhanced_messages = self.context_manager.get_enhanced_messages()
            messages.extend(enhanced_messages)

            # å‡†å¤‡è¯·æ±‚æ•°æ®
            data = {
                "model": self.config.get("model", "gpt-3.5-turbo"),
                "messages": messages,
                "temperature": 0.7,
                "max_tokens": 6000
            }

            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.config.get('api_key', '')}"
            }

            # å¯åŠ¨æ€è€ƒåŠ¨ç”»
            start_thinking()
            
            # å¯åŠ¨ä»»åŠ¡ç›‘æ§
            start_task_monitoring(interrupt_current_task)

            try:
                # å‘é€è¯·æ±‚ï¼Œå¢åŠ è¶…æ—¶æ—¶é—´
                response = requests.post(self.api_url, json=data, headers=headers, timeout=180)
            finally:
                # ç¡®ä¿æ— è®ºå¦‚ä½•éƒ½åœæ­¢åŠ¨ç”»å’Œç›‘æ§
                stop_thinking()
                stop_task_monitoring()

            # æ£€æŸ¥æ˜¯å¦è¢«ä¸­æ–­
            if is_task_interrupted():
                reset_interrupt_flag()
                return "ä»»åŠ¡å·²è¢«ç”¨æˆ·ä¸­æ–­"

            if response.status_code == 401:
                return f"è®¤è¯å¤±è´¥: APIå¯†é’¥æ— æ•ˆæˆ–æœªæˆæƒã€‚è¯·æ£€æŸ¥æ‚¨çš„å¯†é’¥ã€‚ - {response.text}"

            if response.status_code == 200:
                result = response.json()
                ai_response = result['choices'][0]['message']['content']

                # æ·»åŠ AIå“åº”åˆ°ä¸Šä¸‹æ–‡ç®¡ç†å™¨
                self.context_manager.add_message("assistant", ai_response)

                # ä¿æŒå‘åå…¼å®¹çš„å†å²è®°å½•ï¼ˆä½†ç°åœ¨ä¸»è¦ç”±context_managerç®¡ç†ï¼‰
                self.conversation_history.append({"role": "user", "content": user_input})
                self.conversation_history.append({"role": "assistant", "content": ai_response})
                
                # è‡ªåŠ¨ä¿å­˜ä¸Šä¸‹æ–‡
                self.context_manager.save_context()

                # é™åˆ¶å†å²é•¿åº¦ï¼Œä½†ä¿ç•™æ›´å¤šä¸Šä¸‹æ–‡ï¼ˆä¿æŒ20æ¡æ¶ˆæ¯ï¼‰
                if len(self.conversation_history) > 20:
                    self.conversation_history = self.conversation_history[-20:]

                # è¿”å›åŸå§‹å“åº”ï¼Œç”±è°ƒç”¨è€…å†³å®šå¦‚ä½•æ ¼å¼åŒ–
                return ai_response
            else:
                return f"APIè¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}"

        except requests.exceptions.Timeout:
            # ç¡®ä¿åœæ­¢åŠ¨ç”»å’Œç›‘æ§
            try:
                stop_thinking()
                stop_task_monitoring()
            except:
                pass
            return "è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•"
        except requests.exceptions.RequestException as e:
            # ç¡®ä¿åœæ­¢åŠ¨ç”»å’Œç›‘æ§
            try:
                stop_thinking()
                stop_task_monitoring()
            except:
                pass
            return f"ç½‘ç»œé”™è¯¯: {str(e)}"
        except KeyboardInterrupt:
            # å¤„ç†Ctrl+Cä¸­æ–­
            try:
                stop_thinking()
                stop_task_monitoring()
            except:
                pass
            return "ä»»åŠ¡å·²è¢«ç”¨æˆ·ä¸­æ–­"
        except Exception as e:
            # ç¡®ä¿åœæ­¢åŠ¨ç”»å’Œç›‘æ§
            try:
                stop_thinking()
                stop_task_monitoring()
            except:
                pass
            return f"å‘ç”Ÿé”™è¯¯: {str(e)}"

    def clear_history(self):
        """æ¸…é™¤å¯¹è¯å†å²"""
        self.conversation_history = []


    def get_history(self):
        """è·å–å½“å‰å¯¹è¯å†å²"""
        return self.conversation_history

    def set_history(self, history):
        """è®¾ç½®æ–°çš„å¯¹è¯å†å²"""
        self.conversation_history = history

# å…¨å±€AIå®¢æˆ·ç«¯å®ä¾‹
ai_client = AIClient()
